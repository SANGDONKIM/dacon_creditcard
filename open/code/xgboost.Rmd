---
title: "XGboost bayesopt with credit data"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
```

# Preparations {.tabset .tabset-fade}

## Libraries

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(lubridate)
library(skimr)
library(magrittr)
library(data.table)
library(gridExtra)
library(themis)

theme_set(theme_bw())
```

## Data load

```{r}
file_path <- 'C:/Users/uos/Desktop/dacon_creditcard/open/open'
files <- list.files(file_path)
files
```

```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv")) %>% janitor::clean_names()
test <- read_csv(file.path(file_path, "test.csv")) %>% janitor::clean_names()
```

# Data overview (데이터 기본정보) {.tabset .tabset-fade}

## train data
```{r}
head(train)
skim(train)
```

## test data

```{r}
head(test)
skim(test)

test %>% 
  summarise(across(.fns = ~sum(is.na(.))/length(.)))

```

# data preprocessing {.tabset .tabset-fade}

## combine train, test 

```{r}
all_data <- bind_rows(train, test)
```

## Change variable type
```{r}
cols <- colnames(all_data)[c(13:18, 20)]
all_data %>% 
    mutate_if(is.character, as.factor) %>% 
    mutate_at(cols, funs(factor(.))) -> all_data
```

## change date variable 
```{r}
all_data$DAYS_BIRTH %>% summary() # 출생일 
all_data$DAYS_EMPLOYED %>% summary() # 업무 시작일 
all_data$begin_month %>% summary() # 신용카드 발급 월 


all_data %>% 
    mutate(days_birth = days_birth + 25152,
           days_employed = days_employed + 15713, 
           begin_month = begin_month + 60) -> all_data

all_data %>% 
  mutate(birth = round(days_birth/365), 
         month_employed = round(days_employed/30)) %>% 
  select(-c(days_birth, days_employed))-> all_data

head(all_data)
```


# Univariate visualization {.tabset .tabset-fade}



```{r}
table(all_data$gender, useNA = 'always')
table(all_data$car, useNA = 'always')
table(all_data$reality, useNA = 'always')
table(all_data$income_type, useNA = 'always') # label 불균형 
table(all_data$edu_type, useNA = 'always')
table(all_data$family_type, useNA = 'always')
table(all_data$house_type, useNA = 'always') # label 불균형 
table(all_data$flag_mobil, useNA = 'always')
table(all_data$work_phone, useNA = 'always')
table(all_data$phone, useNA = 'always')
table(all_data$email, useNA = 'always') # label 불균형 
table(all_data$occyp_type, useNA = 'always') # label 불균형 
table(all_data$credit)

all_data %>% 
  recipe(credit~.) %>%
  step_rm(flag_mobil) %>% 
  step_other(all_nominal(), threshold = 0.1) %>% 
  step_bagimpute(occyp_type, impute_with = imp_vars(car, reality, income_total, income_type, edu_type, phone),  trees = 100) %>% 
  prep(training = all_data) %>% 
  bake(new_data = all_data) -> all_data
  

```


```{r}
all_data %>% 
  ggplot(aes(x = child_num)) + geom_histogram()
```

```{r}
all_data %>% 
  ggplot(aes(x = income_total)) + geom_histogram()
```
```{r}
all_data %>% 
  ggplot(aes(x = days_birth)) + geom_histogram()
```
```{r}
all_data %>% 
  ggplot(aes(x = days_employed)) + geom_histogram()

table(all_data$days_employed)
```
```{r}
all_data %>% 
  ggplot(aes(x = begin_month)) + geom_histogram()
```


# Recipes
```{r}
all_data %>% 
    recipe(credit~.) %>%
    step_rm(index) %>% 
    #step_discretize(days_employed, breaks = 7) %>% 
    # step_dummy(all_nominal(), one_hot = T) %>% 
    prep(training = all_data) %>%
    bake(new_data = all_data) -> all_data

```


# Split train, test {.tabset .tabset-fade}

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data[train_index,]
test2 <- all_data[-train_index,]

```

# cross validation

```{r}
set.seed(20210410)
vb_folds <- vfold_cv(train2, v = 5, strata = credit)
vb_folds
```

# randomforest setting {.tabset .tabset-fade}

## randomforest hyperparameter setting 
```{r}
rf_spec <- rand_forest(
  mtry = tune(), 
  trees = 1000, 
  min_n = tune()
) %>% 
  set_mode('classification') %>% 
  set_engine('ranger')
```

## workflow model setting

```{r}
rf_wf <- workflow() %>% 
    add_formula(credit~.) %>% 
    add_model(rf_spec)
```


## hyperparameter 튜닝 
```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
set.seed(1234)

rf_res <- tune_grid(
    rf_wf,  
    resamples = vb_folds, 
    grid = 30,
    control = control_grid(save_pred = TRUE)    
)
toc() # 2270.45

```


## Final model update

```{r}
best_param_rf <- select_best(rf_res) # metric 설정안해놓으면 roc_auc로 설정됨 
final_rf <- finalize_workflow(rf_wf, best_param_rf)
final_rf
```

## final model setting

```{r}
final_rf_model <- finalize_model(rf_spec, best_param_rf) 
final_rf_model 
```

## final model workflow에 업데이트

```{r}
final_rf_workflow <- rf_wf %>% update_model(final_rf_model)
```

## final model 학습

```{r}
rf_fit <- fit(final_rf_workflow, data = train2)
```

# Result {.tabset .tabset-fade}

## Prediction 

```{r}
pred_rf <- 
    predict(rf_fit, test2) %>% 
    mutate(modelo = "randomforest")
pred_rf %>% head()

```


# XGboost setting {.tabset .tabset-fade}

## XGBOOST hyperparameter setting

```{r}
xgb_spec <- boost_tree(
    trees = 1000,  
    tree_depth = tune(),  
    min_n = tune(), 
    loss_reduction = tune(),  
    sample_size = tune(), 
    mtry = tune(),  
    learn_rate = tune() 
) %>% 
    set_engine('xgboost') %>% # objective ="multi:softprob", num_class = 3, 
    set_mode('classification')
```

# XGboost workflow {.tabset .tabset-fade}

## workflow model setting

```{r}
xgb_wf <- workflow() %>% 
    add_formula(credit~.) %>% 
    add_model(xgb_spec)
```

## cross validation

```{r}
set.seed(20210409)
vb_folds <- vfold_cv(train2, v = 5)
vb_folds
```

## Grid search 
```{r}
xgb_grid <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train2), # mtry() : [1, ?], finalize(mtry(), train2) : [1, 30]
    learn_rate(), 
    size = 30
)
```


## hyperparameter 튜닝 
```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
set.seed(1234)

xgb_res <- tune_grid(
    xgb_wf,  
    resamples = vb_folds, 
    grid = xgb_grid, 
    control = control_grid(save_pred = TRUE)    
)
toc()  
```

## Final model update

```{r}
best_param <- select_best(xgb_res)
final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb
```

## final model setting

```{r}
final_model <- finalize_model(xgb_spec, best_param) 
final_model # tuning이 끝난 최종 모형 
```

## final model workflow에 업데이트

```{r}
final_workflow <- xgb_wf %>% update_model(final_model)
```

## final model 학습

```{r}
xgb_fit <- fit(final_workflow, data = train2)
```

# Result {.tabset .tabset-fade}

## Prediction 

```{r}
pred_xgb <- 
    predict(xgb_fit, test2) %>% 
    mutate(modelo = "XGBoost")
pred_xgb %>% head()

```

## feature importance plot

```{r}
library(vip) # feature importance plot 그리기 
final_xgb %>% 
    fit(data = train2) %>%  # iter, training_rmse 
    pull_workflow_fit() %>% #  http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
    vip(geom = 'point') # DAYS_BIRTH, DAYS_EMPLOYED, income_total, begin_monthbin5
```


# Submit file {.tabset .tabset-fade}

```{r}
subfile <- read_csv(file.path(file_path, "sample_submission.csv"))

pred_rf %>% 
    recipe(.pred_class~.) %>%
    step_rm(modelo) %>% 
    step_dummy(.pred_class, one_hot = T) %>% 
    prep() %>% 
    bake(new_data = pred_rf) -> pred_result

pred_result

subfile %>% 
  select(index) %>% 
  bind_cols(pred_result) %>% 
  rename('0' = '.pred_class_X0', '1' = '.pred_class_X1', '2' = '.pred_class_X2') -> subfile

write.csv(subfile, row.names = FALSE,
          file.path(file_path, "rf_result.csv")) 
```

