---
title: "XGboost bayesopt with credit data"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
```

# Preparations {.tabset .tabset-fade}

## Libraries

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(lubridate)
library(skimr)
library(magrittr)
library(data.table)
library(gridExtra)
library(themis)
library(ggmosaic)
theme_set(theme_bw())
```

## Data load

```{r}
file_path <- 'C:/Users/sangdon/Desktop/dacon_creditcard/open/open'
files <- list.files(file_path)
files
```

```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv")) %>% janitor::clean_names()
test <- read_csv(file.path(file_path, "test.csv")) %>% janitor::clean_names()
```

# Data overview (데이터 기본정보) {.tabset .tabset-fade}

## train data
```{r}
head(train)
skim(train)
```

## test data

```{r}
head(test)
skim(test)

test %>% 
  summarise(across(.fns = ~sum(is.na(.))/length(.)))

```

# data preprocessing {.tabset .tabset-fade}

## combine train, test 

```{r}
all_data <- bind_rows(train, test)
```

## Change variable type
```{r}
cols <- colnames(all_data)[c(13:18, 20)]
all_data %>% 
    mutate_if(is.character, as.factor) %>% 
    mutate_at(cols, funs(factor(.))) -> all_data
```

## change date variable 

```{r}
all_data %>% 
    mutate(days_birth = days_birth + 25152, # 출생일
           days_employed = days_employed + 15713, # 업무 시작일  
           begin_month = begin_month + 60) -> all_data # 신용카드 발급 월 

all_data %>% 
  mutate(birth = round(days_birth/365), 
         month_employed = round(days_employed/30)) %>% 
  select(-c(days_birth, days_employed))-> all_data

```

```{r}
all_data %>% 
  ggplot(aes(x = birth, y = income_total)) + geom_point()

all_data %>% 
  ggplot(aes(x = begin_month, y = income_total)) + geom_point()

all_data %>% 
  ggplot(aes(x = month_employed, y = income_total)) + geom_point()

all_data %>% 
  ggplot(aes(x = month_employed)) + geom_histogram()

all_data %>% 
  count(month_employed) # month_employed : 12699, count = 6135 
```
```{r}
all_data %>% 
  recipe(credit~.) %>% 
  step_discretize(month_employed, num_breaks = 5) %>% 
  prep(training = all_data) %>% 
  bake(new_data = all_data) -> all_data

```



# Univariate visualization {.tabset .tabset-fade}

```{r}
table(all_data$gender, useNA = 'always')
table(all_data$car, useNA = 'always')
table(all_data$reality, useNA = 'always')
table(all_data$income_type, useNA = 'always') # label 불균형 
table(all_data$edu_type, useNA = 'always')
table(all_data$family_type, useNA = 'always')
table(all_data$house_type, useNA = 'always') # label 불균형 
table(all_data$flag_mobil, useNA = 'always')
table(all_data$work_phone, useNA = 'always')
table(all_data$phone, useNA = 'always')
table(all_data$email, useNA = 'always') # label 불균형 
table(all_data$occyp_type, useNA = 'always') # label 불균형 
table(all_data$credit)
```


## income_type : 소득 분류 
commercial associate, pensioner(연금 수급자), state servant(공무원), student, working 

```{r}
table(all_data$income_type, useNA = 'always')
all_data %>% 
  ggplot(aes(x = income_type, y = income_total)) + geom_boxplot() 

all_data %>% 
  filter(!is.na(income_type), !is.na(credit)) %>% 
  ggplot()+geom_mosaic(aes(x = product(income_type, credit), fill = income_type))

```
## house_type : 생활 방식 
co-op apartment(주택 협동조합), house apartment, municipal apartment(공공 주택), office apartment(회사), rented apartment(임대 주택), with parents

```{r}
all_data %>% 
  ggplot(aes(x = house_type)) + geom_bar() + 
  aes(stringr::str_wrap(house_type, 15)) + 
  xlab('house_type')

all_data %>% 
  filter(!is.na(house_type), !is.na(credit)) %>% 
  ggplot()+geom_mosaic(aes(x = product(house_type, credit), fill = house_type))

```

## email

```{r}
all_data %>% 
  ggplot(aes(x = email)) + geom_bar()

all_data %>% 
  ggplot(aes(x = email, y = income_total, fill = email)) + geom_boxplot()

all_data %>% 
  filter(!is.na(email), !is.na(credit)) %>% 
  ggplot()+geom_mosaic(aes(x = product(email, credit), fill = email))

```

## occupation type

accountants, cleaning staff, cooking staff, core staff(정규직), drivers, high skill tech staff, HR staff, IT staff, Laborers, Low-skill Laborers, managers, medicine staff

```{r}
table(all_data$occyp_type, useNA = 'always') # label 불균형 


all_data %>% 
  ggplot(aes(x = occyp_type)) + 
  geom_bar() + 
  coord_flip()

all_data %>% 
  filter(!is.na(occyp_type), !is.na(credit)) %>% 
  ggplot(aes(x = occyp_type, y = income_total, fill = occyp_type)) + 
  geom_boxplot() + 
  coord_flip()

all_data %>% 
  filter(!is.na(occyp_type), !is.na(credit)) %>% 
  ggplot()+geom_mosaic(aes(x = product(occyp_type, credit), fill =occyp_type))

```

## Preprocessing factor variable 

```{r}
all_data %>% 
  recipe(credit~.) %>%
  step_rm(flag_mobil, email) %>%
  step_other(income_type, occyp_type, house_type, threshold = 0.1) %>% 
  step_bagimpute(occyp_type, impute_with = imp_vars(car, reality, income_total, income_type, edu_type, house_type, family_type, family_size),  trees = 100) %>% 
  prep(training = all_data) %>% 
  bake(new_data = all_data) -> all_data

```


# Recipes
```{r}
all_data %>% 
  recipe(credit~.) %>%
  step_rm(index) %>% 
  #step_integer(all_nominal(), -all_outcomes()) %>% 
  #step_center(all_predictors(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -credit, one_hot = TRUE) %>% 
  prep(training = all_data) %>%
  bake(new_data = all_data) -> all_data

all_data %>% 
  recipe(credit~.) %>% 
  step_smote(credit, over_ratio = 0.5) %>% 
  prep(training = all_data) %>%
  bake(new_data = all_data) -> all_data


```


# Split train, test {.tabset .tabset-fade}

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data[train_index,]
test2 <- all_data[-train_index,]

```

# cross validation

```{r}
set.seed(20210410)
vb_folds <- vfold_cv(train2, v = 5, strata = credit)
vb_folds
```

# randomforest setting {.tabset .tabset-fade}

## randomforest hyperparameter setting 
```{r}
rf_spec <- rand_forest(
  mtry = tune(), 
  trees = 1000, 
  min_n = tune()
) %>% 
  set_mode('classification') %>% 
  set_engine('ranger')
```

## workflow model setting

```{r}
rf_wf <- workflow() %>% 
    add_formula(credit~.) %>% 
    add_model(rf_spec)
```


## hyperparameter 튜닝 
```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
set.seed(1234)

rf_res <- tune_grid(
    rf_wf,  
    resamples = vb_folds, 
    grid = 30,
    metrics = metric_set(mn_log_loss), 
    control = control_grid(save_pred = TRUE)    
)
toc() # 2270.45

```


## Final model update

```{r}
best_param_rf <- select_best(rf_res) # metric 설정안해놓으면 roc_auc로 설정됨 
final_rf <- finalize_workflow(rf_wf, best_param_rf)
final_rf
```

## final model setting

```{r}
final_rf_model <- finalize_model(rf_spec, best_param_rf) 
final_rf_model 
```

## final model workflow에 업데이트

```{r}
final_rf_workflow <- rf_wf %>% update_model(final_rf_model)
```

## final model 학습

```{r}
rf_fit <- fit(final_rf_workflow, data = train2)
```

# Result {.tabset .tabset-fade}

## Prediction 

```{r}
pred_rf <- 
    predict(rf_fit, test2, type = 'prob') %>% 
    mutate(modelo = "randomforest")
pred_rf %>% head()

```


# XGboost setting {.tabset .tabset-fade}

## XGBOOST hyperparameter setting

```{r}
xgb_spec <- boost_tree(
    trees = 1000,  
    tree_depth = tune(),  
    min_n = tune(), 
    loss_reduction = tune(),  
    sample_size = tune(), 
    mtry = tune(),  
    learn_rate = tune() 
) %>% 
    set_engine('xgboost') %>%  
    set_mode('classification')
```

# XGboost workflow {.tabset .tabset-fade}

## workflow model setting

```{r}
xgb_wf <- workflow() %>% 
    add_formula(credit~.) %>% 
    add_model(xgb_spec)
```

## cross validation

```{r}
set.seed(20210409)
vb_folds <- vfold_cv(train2, v = 5)
```

## Grid search 
```{r}
xgb_grid <- grid_latin_hypercube(
    tree_depth(), 
    min_n(), 
    loss_reduction(), 
    sample_size = sample_prop(), 
    finalize(mtry(), train2), 
    learn_rate(), 
    size = 30
)
```


## hyperparameter 튜닝 
```{r}
library(tictoc)
tic()
doParallel::registerDoParallel()
set.seed(1234)

xgb_res <- tune_grid(
    xgb_wf,  
    resamples = vb_folds, 
    grid = xgb_grid, 
    metrics = metric_set(mn_log_loss),
    control = control_grid(save_pred = TRUE)    
)
toc()  
```

## Final model update

```{r}
best_param <- select_best(xgb_res)
final_xgb <- finalize_workflow(xgb_wf, best_param)
final_xgb
```

## final model setting

```{r}
final_model <- finalize_model(xgb_spec, best_param) 
final_model # tuning이 끝난 최종 모형 
```

## final model workflow에 업데이트

```{r}
final_workflow <- xgb_wf %>% update_model(final_model)
```

## final model 학습

```{r}
xgb_fit <- fit(final_workflow, data = train2)
```

# Result {.tabset .tabset-fade}

## Prediction 

```{r}
pred_xgb <- 
    predict(xgb_fit, test2, type = 'prob') 
pred_xgb %>% head()

```

## feature importance plot

```{r}
library(vip) 
final_xgb %>% 
    fit(data = train2) %>%   
    pull_workflow_fit() %>% 
    vip(geom = 'point') 
```
# tabnet 

```{r}
library(tabnet)
all_data %>% 
  recipe(credit~.) %>% 
  step_normalize(all_numeric()) %>% 
  prep(training = all_data) %>%
  bake(new_data = all_data) -> tab_rec

```

```{r}
wflow_tabnet_fit <- 
  workflow() %>% 
  add_model(
    tabnet(
      mode = 'classification', 
      batch_size = 128, 
      virtual_batch_size = 128, 
      epochs = 10
    )
  )
```




# Submit file {.tabset .tabset-fade}


```{r, message=FALSE}
submission <- read_csv(file.path(file_path, "sample_submission.csv"))

sub_col <- names(submission)

submission <- bind_cols(submission$index, pred_xgb)
submission
names(submission) <- sub_col

write.csv(submission, row.names = FALSE,
          "dacon_xgb.csv")
```

